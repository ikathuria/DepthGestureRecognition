{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Motion Gesture Recognition","metadata":{}},{"cell_type":"code","source":"import os\nimport math\nimport gc\nimport pickle\nfrom tqdm import tqdm\nimport cv2\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:56:56.096053Z","iopub.execute_input":"2021-06-07T05:56:56.096636Z","iopub.status.idle":"2021-06-07T05:56:56.26807Z","shell.execute_reply.started":"2021-06-07T05:56:56.096495Z","shell.execute_reply":"2021-06-07T05:56:56.266618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# classes label you want to use all labels \ntargets_name = pd.read_csv('../input/20bnjester/Labels.csv', header=None)\ntargets_name.drop([0, 1, 3, 4, 5, 6, 7, 8, 9, 15, 16, 17, 18, 21, 22, 23, 24, 25, 26], inplace=True)\ntargets_name = targets_name[0].tolist()\ntargets_name","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training targets\ntargets = pd.read_csv('../input/20bnjester/20BN-JESTER/Train.csv', index_col=0).drop(columns=['frames', 'label_id', 'shape', 'format'])\nprint(targets['label'].value_counts())\ntargets.sort_values('label')\ntargets = targets.squeeze().to_dict()\ntargets = {key:val for key, val in targets.items() if val in targets_name}\nprint('Total items:', len(targets))\ntargets","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# validation targets\ntargets_validation = pd.read_csv('../input/20bnjester/20BN-JESTER/Validation.csv', index_col=0).drop(columns=['frames', 'label_id', 'shape', 'format'])\ntargets_validation = targets_validation.squeeze().to_dict()\ntargets_validation = {key:val for key, val in targets_validation.items() if val in targets_name}\nprint('Total items:', len(targets_validation))\ntargets_validation","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing the frames","metadata":{}},{"cell_type":"markdown","source":"1. Unify frames to be 30 in each folder.\n2. Resize the frames to 64x64 for input.\n3. Convert them to grayscale.\n4. Convert the list of frames to an np array.","metadata":{}},{"cell_type":"code","source":"def release_list(a):\n    \"\"\"Function to empty the RAM.\"\"\"\n    del a[:]\n    del a\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hm_frames = 30  # number of frames\ndef get_unify_frames(path):\n    \"\"\"Unify number of frames for each training.\n    \n    Args:\n        path: path to directory.\n    \"\"\"\n    offset = 0\n\n    # pick frames\n    frames = os.listdir(path)\n    frames_count = len(frames)\n\n    if hm_frames > frames_count:\n        # duplicate last frame if video is shorter than necessary\n        frames += [frames[-1]] * (hm_frames - frames_count)\n    elif hm_frames < frames_count:\n        # if there are more frames, then sample starting offset\n        frames = frames[0:hm_frames]\n    return frames","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def resize_frame(frame):\n    \"\"\"Resize frames.\n    \n    Args:\n        frame: image to be resized.\n    \"\"\"\n    frame = cv2.imread(frame)\n    frame = cv2.resize(frame, (64, 64))\n    return frame","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# return gray image\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# main path\npath = \"../input/20bnjester/20BN-JESTER/\"\n\n# training directories\ntemp = {}\ndirs = []\nfor key, val in targets.items():\n    if val not in temp:\n        temp[val] = [key]\n    else:\n        temp[val].append(key)\n\n    if len(temp[val]) <= 500:\n        dirs.append(str(key))\n\n# validation directories\ntemp = {}\ndirs_cv = []\nfor key, val in targets_validation.items():\n    if val not in temp:\n        temp[val] = [key]\n    else:\n        temp[val].append(key)\n\n    if len(temp[val]) <= 62:\n        dirs_cv.append(str(key))\n\n# dirs = [str(i) for i in targets.keys()]\n# dirs_cv = [str(i) for i in targets_validation.keys()]\n\nprint(len(dirs))\nprint(len(dirs_cv))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adjust training data\ncounter_training = 0 # number of training\ntraining_targets = [] # training targets \nnew_frames = [] # training data after resize & unify\n\nfor directory in tqdm(dirs):\n    new_frame = [] # one training\n    # Frames in each folder\n    frames = get_unify_frames(path + 'Train/' + directory)\n    if len(frames) == hm_frames: # just to be sure\n        for frame in frames:\n            frame = resize_frame(path + 'Train/' + directory + '/' + frame)\n            new_frame.append(rgb2gray(frame))\n            if len(new_frame) == 15: # partition each training on two trainings.\n                new_frames.append(new_frame) # append each partition to training data\n                training_targets.append(targets_name.index(targets[int(directory)]))\n                counter_training +=1\n                new_frame = []\n                gc.collect()\n\n\ngc.collect()\n\nwith open('new-frames.pkl', 'wb') as file:\n    pickle.dump(new_frames, file)\nrelease_list(new_frames)\n\nwith open('training-targets.pkl', 'wb') as file:\n    pickle.dump(training_targets, file)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we do the same for the validation data\ncounter_validation = 0\ncv_targets = []\nnew_frames_cv = []\n\nfor directory in tqdm(dirs_cv):\n    new_frame = []\n    # Frames in each folder\n    frames = get_unify_frames(path + 'Validation/' + directory)\n    if len(frames)==hm_frames:\n        for frame in frames:\n            frame = resize_frame(path + 'Validation/' + directory + '/' + frame)\n            new_frame.append(rgb2gray(frame))\n            if len(new_frame) == 15:\n                new_frames_cv.append(new_frame)\n                cv_targets.append(targets_name.index(targets_validation[int(directory)]))\n                counter_validation +=1\n                new_frame = []\nprint(counter_validation)\n\ngc.collect()\n\nwith open('cv-new-frames.pkl', 'wb') as file:\n    pickle.dump(new_frames_cv, file)\nrelease_list(new_frames_cv)\n\nwith open('cv-targets.pkl', 'wb') as file:\n    pickle.dump(cv_targets, file)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counter_training = 4000*2\nprint(counter_training)\ncounter_validation = 496*2\nprint(counter_validation)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:57:03.263335Z","iopub.execute_input":"2021-06-07T05:57:03.263767Z","iopub.status.idle":"2021-06-07T05:57:03.270141Z","shell.execute_reply.started":"2021-06-07T05:57:03.26373Z","shell.execute_reply":"2021-06-07T05:57:03.268995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training\nwith open('../input/20bnjester/training-targets.pkl', 'rb') as file:\n    training_targets = pickle.load(file)\n\nwith open('../input/20bnjester/new-frames.pkl', 'rb') as file:\n    new_frames = pickle.load(file)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:57:03.768477Z","iopub.execute_input":"2021-06-07T05:57:03.768938Z","iopub.status.idle":"2021-06-07T05:57:43.790426Z","shell.execute_reply.started":"2021-06-07T05:57:03.768899Z","shell.execute_reply":"2021-06-07T05:57:43.787117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# validation\nwith open('../input/20bnjester/cv-targets.pkl', 'rb') as file:\n    cv_targets = pickle.load(file)\n\nwith open('../input/20bnjester/cv-new-frames.pkl', 'rb') as file:\n    new_frames_cv = pickle.load(file)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-07T05:57:43.79491Z","iopub.execute_input":"2021-06-07T05:57:43.795783Z","iopub.status.idle":"2021-06-07T05:57:49.500554Z","shell.execute_reply.started":"2021-06-07T05:57:43.795702Z","shell.execute_reply":"2021-06-07T05:57:49.499363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert training data to np float32\ntraining_data = np.array(new_frames[0:counter_training], dtype=np.float32)\ntraining_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:57:49.502878Z","iopub.execute_input":"2021-06-07T05:57:49.503332Z","iopub.status.idle":"2021-06-07T05:58:07.325423Z","shell.execute_reply.started":"2021-06-07T05:57:49.503286Z","shell.execute_reply":"2021-06-07T05:58:07.323943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert validation data to np float32\ncv_data = np.array(new_frames_cv[0:counter_validation], dtype=np.float32)\ncv_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:58:07.327697Z","iopub.execute_input":"2021-06-07T05:58:07.328217Z","iopub.status.idle":"2021-06-07T05:58:08.042916Z","shell.execute_reply.started":"2021-06-07T05:58:07.328165Z","shell.execute_reply":"2021-06-07T05:58:08.041426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To check training length\nprint(\"Training new frames:\", len(training_data))\n\n# To check validation length\nprint(\"Validation new frames:\", len(cv_data))","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:58:08.045188Z","iopub.execute_input":"2021-06-07T05:58:08.0462Z","iopub.status.idle":"2021-06-07T05:58:08.055887Z","shell.execute_reply.started":"2021-06-07T05:58:08.046122Z","shell.execute_reply":"2021-06-07T05:58:08.054017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:58:08.058384Z","iopub.execute_input":"2021-06-07T05:58:08.059722Z","iopub.status.idle":"2021-06-07T05:58:08.333519Z","shell.execute_reply.started":"2021-06-07T05:58:08.059651Z","shell.execute_reply":"2021-06-07T05:58:08.332124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Normalization","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\ndef normalization(data):\n    print('old mean', data.mean())\n\n    scaler = StandardScaler()\n\n    scaled_images  = scaler.fit_transform(data.reshape(-1, 15*64*64))\n    print('new mean', scaled_images.mean())\n    \n    scaled_images  = scaled_images.reshape(-1, 15, 64, 64, 1)    \n    print(scaled_images.shape)\n    \n    return scaled_images","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:58:08.335075Z","iopub.execute_input":"2021-06-07T05:58:08.335431Z","iopub.status.idle":"2021-06-07T05:58:09.515579Z","shell.execute_reply.started":"2021-06-07T05:58:08.335393Z","shell.execute_reply":"2021-06-07T05:58:09.514285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalisation: training\nscaled_images = normalization(training_data)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:58:09.519567Z","iopub.execute_input":"2021-06-07T05:58:09.520161Z","iopub.status.idle":"2021-06-07T05:58:20.045866Z","shell.execute_reply.started":"2021-06-07T05:58:09.520103Z","shell.execute_reply":"2021-06-07T05:58:20.04456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalisation: validation\nscaled_images_cv = normalization(cv_data)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:58:20.048091Z","iopub.execute_input":"2021-06-07T05:58:20.048423Z","iopub.status.idle":"2021-06-07T05:58:21.259881Z","shell.execute_reply.started":"2021-06-07T05:58:20.048389Z","shell.execute_reply":"2021-06-07T05:58:21.258634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating and training the model","metadata":{}},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:58:21.261403Z","iopub.execute_input":"2021-06-07T05:58:21.261719Z","iopub.status.idle":"2021-06-07T05:58:21.439841Z","shell.execute_reply.started":"2021-06-07T05:58:21.261685Z","shell.execute_reply":"2021-06-07T05:58:21.43845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:58:21.44155Z","iopub.execute_input":"2021-06-07T05:58:21.441896Z","iopub.status.idle":"2021-06-07T05:58:28.069183Z","shell.execute_reply.started":"2021-06-07T05:58:21.44186Z","shell.execute_reply":"2021-06-07T05:58:28.068142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import Model\nfrom keras.layers import Conv3D, MaxPool3D, ConvLSTM2D, Flatten, Dense","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:58:28.070839Z","iopub.execute_input":"2021-06-07T05:58:28.07158Z","iopub.status.idle":"2021-06-07T05:58:28.146624Z","shell.execute_reply.started":"2021-06-07T05:58:28.071527Z","shell.execute_reply":"2021-06-07T05:58:28.1456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Conv3DModel(Model):\n    def __init__(self):\n        super(Conv3DModel, self).__init__()\n#         with tpu_strategy.scope():\n        # Convolutions\n        self.conv1 = Conv3D(32, (3, 3, 3), activation='relu', name=\"conv1\", data_format='channels_last')\n        self.pool1 = MaxPool3D(pool_size=(2, 2, 2), data_format='channels_last')\n        self.conv2 = Conv3D(64, (3, 3, 3), activation='relu', name=\"conv1\", data_format='channels_last')\n        self.pool2 = MaxPool3D(pool_size=(2, 2,2), data_format='channels_last')\n\n        # LSTM & Flatten\n        self.convLSTM = ConvLSTM2D(40, (3, 3))\n        self.flatten = Flatten(name=\"flatten\")\n\n        # Dense layers\n        self.d1 = Dense(128, activation='relu', name=\"d1\")\n        self.out = Dense(8, activation='softmax', name=\"output\")\n\n    def call(self, x):\n#         with tpu_strategy.scope():\n        x = self.conv1(x)\n        x = self.pool1(x)\n        x = self.conv2(x)\n        x = self.pool2(x)\n        x = self.convLSTM(x)\n        #x = self.pool2(x)\n        #x = self.conv3(x)\n        #x = self.pool3(x)\n        x = self.flatten(x)\n        x = self.d1(x)\n        return self.out(x)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:58:28.147949Z","iopub.execute_input":"2021-06-07T05:58:28.148523Z","iopub.status.idle":"2021-06-07T05:58:28.160498Z","shell.execute_reply.started":"2021-06-07T05:58:28.148484Z","shell.execute_reply":"2021-06-07T05:58:28.159141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with tpu_strategy.scope():\nmodel = Conv3DModel()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:58:28.163241Z","iopub.execute_input":"2021-06-07T05:58:28.163841Z","iopub.status.idle":"2021-06-07T05:58:28.243133Z","shell.execute_reply.started":"2021-06-07T05:58:28.163796Z","shell.execute_reply":"2021-06-07T05:58:28.242086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use tensorflow dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices((scaled_images, training_targets))\ncv_dataset = tf.data.Dataset.from_tensor_slices((scaled_images_cv, cv_targets))","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:58:28.244773Z","iopub.execute_input":"2021-06-07T05:58:28.245398Z","iopub.status.idle":"2021-06-07T05:58:32.146205Z","shell.execute_reply.started":"2021-06-07T05:58:28.24536Z","shell.execute_reply":"2021-06-07T05:58:32.145071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model(scaled_images[0:2])","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:58:32.14785Z","iopub.execute_input":"2021-06-07T05:58:32.148568Z","iopub.status.idle":"2021-06-07T05:58:32.477572Z","shell.execute_reply.started":"2021-06-07T05:58:32.148522Z","shell.execute_reply":"2021-06-07T05:58:32.476546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-07T05:58:32.479191Z","iopub.execute_input":"2021-06-07T05:58:32.479849Z","iopub.status.idle":"2021-06-07T05:58:32.49275Z","shell.execute_reply.started":"2021-06-07T05:58:32.479806Z","shell.execute_reply":"2021-06-07T05:58:32.491264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metrics","metadata":{}},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:58:32.494565Z","iopub.execute_input":"2021-06-07T05:58:32.494979Z","iopub.status.idle":"2021-06-07T05:58:32.778929Z","shell.execute_reply.started":"2021-06-07T05:58:32.494922Z","shell.execute_reply":"2021-06-07T05:58:32.777756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.losses import SparseCategoricalCrossentropy\nfrom keras.optimizers import Adam\nfrom keras.metrics import Mean, SparseCategoricalAccuracy","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:58:32.780754Z","iopub.execute_input":"2021-06-07T05:58:32.781206Z","iopub.status.idle":"2021-06-07T05:58:32.79403Z","shell.execute_reply.started":"2021-06-07T05:58:32.781151Z","shell.execute_reply":"2021-06-07T05:58:32.792561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_fn = SparseCategoricalCrossentropy()\noptimizer = Adam()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:58:32.796123Z","iopub.execute_input":"2021-06-07T05:58:32.796684Z","iopub.status.idle":"2021-06-07T05:58:32.809456Z","shell.execute_reply.started":"2021-06-07T05:58:32.79664Z","shell.execute_reply":"2021-06-07T05:58:32.807893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss\ntrain_loss = Mean(name='train_loss')\nvalid_loss = Mean(name='valid_loss')\n# Accuracy\ntrain_accuracy = SparseCategoricalAccuracy(name='train_accuracy')\nvalid_accuracy = SparseCategoricalAccuracy(name='valid_accuracy')","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:58:32.811425Z","iopub.execute_input":"2021-06-07T05:58:32.812016Z","iopub.status.idle":"2021-06-07T05:58:32.854706Z","shell.execute_reply.started":"2021-06-07T05:58:32.811864Z","shell.execute_reply":"2021-06-07T05:58:32.853505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_step(image, targets):\n    with tf.GradientTape() as tape:\n        # Make a prediction on all the batch\n        predictions = model(image)\n        # Get the error/loss on these predictions\n        loss = loss_fn(targets, predictions)\n    # Compute the gradient which respect to the loss\n    grads = tape.gradient(loss, model.trainable_variables)\n    # Change the weights of the model\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n    # The metrics are accumulate over time. You don't need to average it yourself.\n    train_loss(loss)\n    train_accuracy(targets, predictions)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:58:32.85636Z","iopub.execute_input":"2021-06-07T05:58:32.856713Z","iopub.status.idle":"2021-06-07T05:58:32.86648Z","shell.execute_reply.started":"2021-06-07T05:58:32.856674Z","shell.execute_reply":"2021-06-07T05:58:32.86518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef valid_step(image, targets):\n    predictions = model(image)\n    t_loss = loss_fn(targets, predictions)\n    # Set the metrics for the test\n    valid_loss(t_loss)\n    valid_accuracy(targets, predictions)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:58:32.870309Z","iopub.execute_input":"2021-06-07T05:58:32.870748Z","iopub.status.idle":"2021-06-07T05:58:32.87774Z","shell.execute_reply.started":"2021-06-07T05:58:32.870701Z","shell.execute_reply":"2021-06-07T05:58:32.876379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, model=model)\nmanager = tf.train.CheckpointManager(ckpt, 'training_checkpoints/tf_ckpts', max_to_keep=10)\nckpt.restore(manager.latest_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:58:32.879565Z","iopub.execute_input":"2021-06-07T05:58:32.880025Z","iopub.status.idle":"2021-06-07T05:58:32.89966Z","shell.execute_reply.started":"2021-06-07T05:58:32.879984Z","shell.execute_reply":"2021-06-07T05:58:32.898261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epoch = 10\nbatch_size = 32\nb = 0\ntraining_acc = []\nvalidation_acc = []\nfor epoch in range(epoch):\n    # Training set\n    for images_batch, targets_batch in train_dataset.batch(batch_size):\n        train_step(images_batch, targets_batch)\n        template = '\\r Batch {}/{}, Loss: {}, Accuracy: {}'\n        print(template.format(\n            b, len(training_targets), train_loss.result(), \n            train_accuracy.result()*100\n        ), end=\"\")\n        b += batch_size\n    # Validation set\n    for images_batch, targets_batch in cv_dataset.batch(batch_size):\n        valid_step(images_batch, targets_batch)\n\n    template = '\\nEpoch {}, Valid Loss: {}, Valid Accuracy: {}'\n    print(template.format(\n        epoch+1,\n        valid_loss.result(), \n        valid_accuracy.result()*100)\n    )\n    training_acc.append(float(train_accuracy.result()*100))\n    validation_acc.append(float(valid_accuracy.result()*100))\n    ckpt.step.assign_add(1)\n    save_path = manager.save()\n    print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))\n    valid_loss.reset_states()\n    valid_accuracy.reset_states()\n    train_accuracy.reset_states()\n    train_loss.reset_states()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:58:32.901307Z","iopub.execute_input":"2021-06-07T05:58:32.901648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(manager.checkpoints)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the model for use in the application\nmodel.save_weights('weights/path_to_my_weights', save_format='tf')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plote Accuracy / epoch\nplt.plot([1,2,3,4,5,6,7,8,9,10],training_acc, '-' )\nplt.plot([1,2,3,4,5,6,7,8,9,10],validation_acc, '-' )\n\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}