{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Motion Gesture Recognition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "import math\r\n",
    "import gc\r\n",
    "import pickle\r\n",
    "from tqdm import tqdm\r\n",
    "import cv2\r\n",
    "import numpy as np\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:56:56.096636Z",
     "iopub.status.busy": "2021-06-07T05:56:56.096053Z",
     "iopub.status.idle": "2021-06-07T05:56:56.26807Z",
     "shell.execute_reply": "2021-06-07T05:56:56.266618Z",
     "shell.execute_reply.started": "2021-06-07T05:56:56.096495Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# classes label you want to use all labels \r\n",
    "targets_name = pd.read_csv('Labels.csv', header=None)\r\n",
    "targets_name.drop([0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 18, 21, 22, 23, 24, 25, 26], inplace=True)\r\n",
    "targets_name = targets_name[0].tolist()\r\n",
    "targets_name"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['No gesture',\n",
       " 'Stop Sign',\n",
       " 'Swiping Left',\n",
       " 'Swiping Right',\n",
       " 'Thumb Down',\n",
       " 'Thumb Up']"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# training targets\r\n",
    "targets = pd.read_csv('Train.csv', index_col=0).drop(columns=['frames', 'label_id', 'shape', 'format'])\r\n",
    "\r\n",
    "for key, value in targets['label'].value_counts().items():\r\n",
    "    if key in targets_name:\r\n",
    "        print(value, key)\r\n",
    "\r\n",
    "targets.sort_values('label')\r\n",
    "targets = targets.squeeze().to_dict()\r\n",
    "targets = {key:val for key, val in targets.items() if val in targets_name}\r\n",
    "\r\n",
    "print('\\n\\nTotal items for {0} gestures: {1}'.format(len(targets_name), len(targets)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1844 No gesture\n",
      "1841 Thumb Up\n",
      "1821 Stop Sign\n",
      "1810 Thumb Down\n",
      "1762 Swiping Left\n",
      "1730 Swiping Right\n",
      "\n",
      "\n",
      "Total items for 6 gestures: 10808\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# validation targets\r\n",
    "targets_validation = pd.read_csv('Validation.csv', index_col=0).drop(columns=['frames', 'label_id', 'shape', 'format'])\r\n",
    "\r\n",
    "for key, value in targets_validation['label'].value_counts().items():\r\n",
    "    if key in targets_name:\r\n",
    "        print(value, key)\r\n",
    "\r\n",
    "targets_validation = targets_validation.squeeze().to_dict()\r\n",
    "targets_validation = {key:val for key, val in targets_validation.items() if val in targets_name}\r\n",
    "\r\n",
    "print('\\n\\nTotal items for {0} gestures: {1}'.format(len(targets_name), len(targets_validation)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "259 Stop Sign\n",
      "256 No gesture\n",
      "250 Thumb Down\n",
      "247 Swiping Left\n",
      "238 Thumb Up\n",
      "231 Swiping Right\n",
      "\n",
      "\n",
      "Total items for 6 gestures: 1481\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing the frames"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Unify frames to be 30 in each folder.\n",
    "2. Resize the frames to 64x64 for input.\n",
    "3. Convert them to grayscale.\n",
    "4. Convert the list of frames to an np array."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def release_list(a):\r\n",
    "    \"\"\"Function to empty the RAM.\"\"\"\r\n",
    "    del a[:]\r\n",
    "    del a\r\n",
    "    gc.collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "hm_frames = 30  # number of frames\r\n",
    "def get_unify_frames(path):\r\n",
    "    \"\"\"Unify number of frames for each training.\r\n",
    "    \r\n",
    "    Args:\r\n",
    "        path: path to directory.\r\n",
    "    \"\"\"\r\n",
    "    offset = 0\r\n",
    "\r\n",
    "    # pick frames\r\n",
    "    frames = os.listdir(path)\r\n",
    "    frames_count = len(frames)\r\n",
    "\r\n",
    "    if hm_frames > frames_count:\r\n",
    "        # duplicate last frame if video is shorter than necessary\r\n",
    "        frames += [frames[-1]] * (hm_frames - frames_count)\r\n",
    "    elif hm_frames < frames_count:\r\n",
    "        # if there are more frames, then sample starting offset\r\n",
    "        frames = frames[0:hm_frames]\r\n",
    "    return frames"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def resize_frame(frame):\r\n",
    "    \"\"\"Resize frames.\r\n",
    "    \r\n",
    "    Args:\r\n",
    "        frame: image to be resized.\r\n",
    "    \"\"\"\r\n",
    "    frame = cv2.imread(frame)\r\n",
    "    frame = cv2.resize(frame, (64, 64))\r\n",
    "    return frame"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# return gray image\r\n",
    "def rgb2gray(rgb):\r\n",
    "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# training directories\r\n",
    "temp = {}\r\n",
    "dirs = []\r\n",
    "for key, val in targets.items():\r\n",
    "    if val not in temp:\r\n",
    "        temp[val] = [key]\r\n",
    "    else:\r\n",
    "        temp[val].append(key)\r\n",
    "\r\n",
    "#     if len(temp[val]) <= 1000:\r\n",
    "    dirs.append(str(key))\r\n",
    "\r\n",
    "# validation directories\r\n",
    "temp = {}\r\n",
    "dirs_cv = []\r\n",
    "for key, val in targets_validation.items():\r\n",
    "    if val not in temp:\r\n",
    "        temp[val] = [key]\r\n",
    "    else:\r\n",
    "        temp[val].append(key)\r\n",
    "\r\n",
    "#     if len(temp[val]) <= 124:\r\n",
    "    dirs_cv.append(str(key))\r\n",
    "\r\n",
    "# dirs = [str(i) for i in targets.keys()]\r\n",
    "# dirs_cv = [str(i) for i in targets_validation.keys()]\r\n",
    "\r\n",
    "print(len(dirs))\r\n",
    "print(len(dirs_cv))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10808\n",
      "1481\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "gc.collect()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Adjust training data\r\n",
    "counter_training = 0 # number of training\r\n",
    "training_targets = [] # training targets \r\n",
    "new_frames = [] # training data after resize & unify\r\n",
    "\r\n",
    "for directory in tqdm(dirs):\r\n",
    "    new_frame = [] # one training\r\n",
    "    # Frames in each folder\r\n",
    "    frames = get_unify_frames('Train/' + directory)\r\n",
    "    if len(frames) == hm_frames: # just to be sure\r\n",
    "        for frame in frames:\r\n",
    "            frame = resize_frame('Train/' + directory + '/' + frame)\r\n",
    "            new_frame.append(rgb2gray(frame))\r\n",
    "            if len(new_frame) == 15: # partition each training on two trainings.\r\n",
    "                new_frames.append(new_frame) # append each partition to training data\r\n",
    "                training_targets.append(targets_name.index(targets[int(directory)]))\r\n",
    "                counter_training +=1\r\n",
    "                new_frame = []\r\n",
    "                gc.collect()\r\n",
    "\r\n",
    "\r\n",
    "gc.collect()\r\n",
    "\r\n",
    "# with open('new-frames.pkl', 'wb') as file:\r\n",
    "#     pickle.dump(new_frames, file)\r\n",
    "# release_list(new_frames)\r\n",
    "\r\n",
    "# with open('training-targets.pkl', 'wb') as file:\r\n",
    "#     pickle.dump(training_targets, file)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 10808/10808 [43:17<00:00,  4.16it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# we do the same for the validation data\r\n",
    "counter_validation = 0\r\n",
    "cv_targets = []\r\n",
    "new_frames_cv = []\r\n",
    "\r\n",
    "for directory in tqdm(dirs_cv):\r\n",
    "    new_frame = []\r\n",
    "    # Frames in each folder\r\n",
    "    frames = get_unify_frames('Validation/' + directory)\r\n",
    "    if len(frames)==hm_frames:\r\n",
    "        for frame in frames:\r\n",
    "            frame = resize_frame('Validation/' + directory + '/' + frame)\r\n",
    "            new_frame.append(rgb2gray(frame))\r\n",
    "            if len(new_frame) == 15:\r\n",
    "                new_frames_cv.append(new_frame)\r\n",
    "                cv_targets.append(targets_name.index(targets_validation[int(directory)]))\r\n",
    "                counter_validation +=1\r\n",
    "                new_frame = []\r\n",
    "print(counter_validation)\r\n",
    "\r\n",
    "gc.collect()\r\n",
    "\r\n",
    "# with open('cv-new-frames.pkl', 'wb') as file:\r\n",
    "#     pickle.dump(new_frames_cv, file)\r\n",
    "# release_list(new_frames_cv)\r\n",
    "\r\n",
    "# with open('cv-targets.pkl', 'wb') as file:\r\n",
    "#     pickle.dump(cv_targets, file)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1481/1481 [01:40<00:00, 14.77it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2962\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "gc.collect()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "counter_training = len(dirs)*2\r\n",
    "print(counter_training)\r\n",
    "\r\n",
    "counter_validation = len(dirs_cv)*2\r\n",
    "print(counter_validation)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "21616\n",
      "2962\n"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:57:03.263767Z",
     "iopub.status.busy": "2021-06-07T05:57:03.263335Z",
     "iopub.status.idle": "2021-06-07T05:57:03.270141Z",
     "shell.execute_reply": "2021-06-07T05:57:03.268995Z",
     "shell.execute_reply.started": "2021-06-07T05:57:03.26373Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# training\r\n",
    "# with open('../input/20bnjester/training-targets.pkl', 'rb') as file:\r\n",
    "#     training_targets = pickle.load(file)\r\n",
    "\r\n",
    "# with open('../input/20bnjester/new-frames.pkl', 'rb') as file:\r\n",
    "#     new_frames = pickle.load(file)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:57:03.768938Z",
     "iopub.status.busy": "2021-06-07T05:57:03.768477Z",
     "iopub.status.idle": "2021-06-07T05:57:43.790426Z",
     "shell.execute_reply": "2021-06-07T05:57:43.787117Z",
     "shell.execute_reply.started": "2021-06-07T05:57:03.768899Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# validation\r\n",
    "# with open('../input/20bnjester/cv-targets.pkl', 'rb') as file:\r\n",
    "#     cv_targets = pickle.load(file)\r\n",
    "\r\n",
    "# with open('../input/20bnjester/cv-new-frames.pkl', 'rb') as file:\r\n",
    "#     new_frames_cv = pickle.load(file)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:57:43.795783Z",
     "iopub.status.busy": "2021-06-07T05:57:43.79491Z",
     "iopub.status.idle": "2021-06-07T05:57:49.500554Z",
     "shell.execute_reply": "2021-06-07T05:57:49.499363Z",
     "shell.execute_reply.started": "2021-06-07T05:57:43.795702Z"
    },
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# convert training data to np float32\r\n",
    "training_data = np.array(new_frames[0:counter_training], dtype=np.float32)\r\n",
    "training_data.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(21616, 15, 64, 64)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:57:49.503332Z",
     "iopub.status.busy": "2021-06-07T05:57:49.502878Z",
     "iopub.status.idle": "2021-06-07T05:58:07.325423Z",
     "shell.execute_reply": "2021-06-07T05:58:07.323943Z",
     "shell.execute_reply.started": "2021-06-07T05:57:49.503286Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# convert validation data to np float32\r\n",
    "cv_data = np.array(new_frames_cv[0:counter_validation], dtype=np.float32)\r\n",
    "cv_data.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2962, 15, 64, 64)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:58:07.328217Z",
     "iopub.status.busy": "2021-06-07T05:58:07.327697Z",
     "iopub.status.idle": "2021-06-07T05:58:08.042916Z",
     "shell.execute_reply": "2021-06-07T05:58:08.041426Z",
     "shell.execute_reply.started": "2021-06-07T05:58:07.328165Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# To check training length\r\n",
    "print(\"Training new frames:\", len(training_data))\r\n",
    "\r\n",
    "# To check validation length\r\n",
    "print(\"Validation new frames:\", len(cv_data))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training new frames: 21616\n",
      "Validation new frames: 2962\n"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:58:08.0462Z",
     "iopub.status.busy": "2021-06-07T05:58:08.045188Z",
     "iopub.status.idle": "2021-06-07T05:58:08.055887Z",
     "shell.execute_reply": "2021-06-07T05:58:08.054017Z",
     "shell.execute_reply.started": "2021-06-07T05:58:08.046122Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "gc.collect()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:58:08.059722Z",
     "iopub.status.busy": "2021-06-07T05:58:08.058384Z",
     "iopub.status.idle": "2021-06-07T05:58:08.333519Z",
     "shell.execute_reply": "2021-06-07T05:58:08.332124Z",
     "shell.execute_reply.started": "2021-06-07T05:58:08.059651Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normalization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "from sklearn.preprocessing import StandardScaler\r\n",
    "def normalization(data):\r\n",
    "    print('old mean', data.mean())\r\n",
    "\r\n",
    "    scaler = StandardScaler()\r\n",
    "\r\n",
    "    scaled_images  = scaler.fit_transform(data.reshape(-1, 15*64*64))\r\n",
    "    print('new mean', scaled_images.mean())\r\n",
    "    \r\n",
    "    scaled_images  = scaled_images.reshape(-1, 15, 64, 64, 1)    \r\n",
    "    print(scaled_images.shape)\r\n",
    "    \r\n",
    "    return scaled_images"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:58:08.335431Z",
     "iopub.status.busy": "2021-06-07T05:58:08.335075Z",
     "iopub.status.idle": "2021-06-07T05:58:09.515579Z",
     "shell.execute_reply": "2021-06-07T05:58:09.514285Z",
     "shell.execute_reply.started": "2021-06-07T05:58:08.335393Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# Normalisation: training\r\n",
    "scaled_images = normalization(training_data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "old mean 112.603355\n",
      "new mean 5.2508855e-09\n",
      "(21616, 15, 64, 64, 1)\n"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:58:09.520161Z",
     "iopub.status.busy": "2021-06-07T05:58:09.519567Z",
     "iopub.status.idle": "2021-06-07T05:58:20.045866Z",
     "shell.execute_reply": "2021-06-07T05:58:20.04456Z",
     "shell.execute_reply.started": "2021-06-07T05:58:09.520103Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# Normalisation: validation\r\n",
    "scaled_images_cv = normalization(cv_data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "old mean 109.66508\n",
      "new mean -7.499211e-10\n",
      "(2962, 15, 64, 64, 1)\n"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:58:20.048423Z",
     "iopub.status.busy": "2021-06-07T05:58:20.048091Z",
     "iopub.status.idle": "2021-06-07T05:58:21.259881Z",
     "shell.execute_reply": "2021-06-07T05:58:21.258634Z",
     "shell.execute_reply.started": "2021-06-07T05:58:20.048389Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating and training the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "gc.collect()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:58:21.261719Z",
     "iopub.status.busy": "2021-06-07T05:58:21.261403Z",
     "iopub.status.idle": "2021-06-07T05:58:21.439841Z",
     "shell.execute_reply": "2021-06-07T05:58:21.43845Z",
     "shell.execute_reply.started": "2021-06-07T05:58:21.261685Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "import tensorflow as tf"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:58:21.441896Z",
     "iopub.status.busy": "2021-06-07T05:58:21.44155Z",
     "iopub.status.idle": "2021-06-07T05:58:28.069183Z",
     "shell.execute_reply": "2021-06-07T05:58:28.068142Z",
     "shell.execute_reply.started": "2021-06-07T05:58:21.44186Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "from tensorflow.keras import Model\r\n",
    "from tensorflow.keras.layers import Conv3D, MaxPool3D, ConvLSTM2D, Flatten, Dense"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:58:28.07158Z",
     "iopub.status.busy": "2021-06-07T05:58:28.070839Z",
     "iopub.status.idle": "2021-06-07T05:58:28.146624Z",
     "shell.execute_reply": "2021-06-07T05:58:28.1456Z",
     "shell.execute_reply.started": "2021-06-07T05:58:28.071527Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "class Conv3DModel(Model):\r\n",
    "    def __init__(self):\r\n",
    "        super(Conv3DModel, self).__init__()\r\n",
    "#         with tpu_strategy.scope():\r\n",
    "        # Convolutions\r\n",
    "        self.conv1 = Conv3D(32, (3, 3, 3), activation='relu', name=\"conv1\", data_format='channels_last')\r\n",
    "        self.pool1 = MaxPool3D(pool_size=(2, 2, 2), data_format='channels_last')\r\n",
    "        self.conv2 = Conv3D(64, (3, 3, 3), activation='relu', name=\"conv1\", data_format='channels_last')\r\n",
    "        self.pool2 = MaxPool3D(pool_size=(2, 2,2), data_format='channels_last')\r\n",
    "\r\n",
    "        # LSTM & Flatten\r\n",
    "        self.convLSTM = ConvLSTM2D(40, (3, 3))\r\n",
    "        self.flatten = Flatten(name=\"flatten\")\r\n",
    "\r\n",
    "        # Dense layers\r\n",
    "        self.d1 = Dense(128, activation='relu', name=\"d1\")\r\n",
    "        self.out = Dense(6, activation='softmax', name=\"output\")\r\n",
    "\r\n",
    "    def call(self, x):\r\n",
    "#         with tpu_strategy.scope():\r\n",
    "        x = self.conv1(x)\r\n",
    "        x = self.pool1(x)\r\n",
    "        x = self.conv2(x)\r\n",
    "        x = self.pool2(x)\r\n",
    "        x = self.convLSTM(x)\r\n",
    "        #x = self.pool2(x)\r\n",
    "        #x = self.conv3(x)\r\n",
    "        #x = self.pool3(x)\r\n",
    "        x = self.flatten(x)\r\n",
    "        x = self.d1(x)\r\n",
    "        return self.out(x)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:58:28.148523Z",
     "iopub.status.busy": "2021-06-07T05:58:28.147949Z",
     "iopub.status.idle": "2021-06-07T05:58:28.160498Z",
     "shell.execute_reply": "2021-06-07T05:58:28.159141Z",
     "shell.execute_reply.started": "2021-06-07T05:58:28.148484Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# with tpu_strategy.scope():\r\n",
    "model = Conv3DModel()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:58:28.163841Z",
     "iopub.status.busy": "2021-06-07T05:58:28.163241Z",
     "iopub.status.idle": "2021-06-07T05:58:28.243133Z",
     "shell.execute_reply": "2021-06-07T05:58:28.242086Z",
     "shell.execute_reply.started": "2021-06-07T05:58:28.163796Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# use tensorflow dataset\r\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((scaled_images, training_targets))\r\n",
    "cv_dataset = tf.data.Dataset.from_tensor_slices((scaled_images_cv, cv_targets))"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:58:28.245398Z",
     "iopub.status.busy": "2021-06-07T05:58:28.244773Z",
     "iopub.status.idle": "2021-06-07T05:58:32.146205Z",
     "shell.execute_reply": "2021-06-07T05:58:32.145071Z",
     "shell.execute_reply.started": "2021-06-07T05:58:28.24536Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "model(scaled_images[0:2])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8), dtype=float32, numpy=\n",
       "array([[0.12232607, 0.12196245, 0.12760179, 0.12782602, 0.12959212,\n",
       "        0.12371509, 0.12133756, 0.12563892],\n",
       "       [0.12215395, 0.12343498, 0.12530209, 0.12717582, 0.12965454,\n",
       "        0.12555675, 0.12020288, 0.12651904]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:58:32.148568Z",
     "iopub.status.busy": "2021-06-07T05:58:32.14785Z",
     "iopub.status.idle": "2021-06-07T05:58:32.477572Z",
     "shell.execute_reply": "2021-06-07T05:58:32.476546Z",
     "shell.execute_reply.started": "2021-06-07T05:58:32.148522Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"conv3d_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv3D)               multiple                  896       \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv3D)               multiple                  55360     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d (ConvLSTM2D)    multiple                  149920    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "d1 (Dense)                   multiple                  737408    \n",
      "_________________________________________________________________\n",
      "output (Dense)               multiple                  1032      \n",
      "=================================================================\n",
      "Total params: 944,616\n",
      "Trainable params: 944,616\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:58:32.479849Z",
     "iopub.status.busy": "2021-06-07T05:58:32.479191Z",
     "iopub.status.idle": "2021-06-07T05:58:32.49275Z",
     "shell.execute_reply": "2021-06-07T05:58:32.491264Z",
     "shell.execute_reply.started": "2021-06-07T05:58:32.479806Z"
    },
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='.\\\\model.png', show_shapes=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metrics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "gc.collect()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:58:32.494979Z",
     "iopub.status.busy": "2021-06-07T05:58:32.494565Z",
     "iopub.status.idle": "2021-06-07T05:58:32.778929Z",
     "shell.execute_reply": "2021-06-07T05:58:32.777756Z",
     "shell.execute_reply.started": "2021-06-07T05:58:32.494922Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Mean, SparseCategoricalAccuracy"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:58:32.781206Z",
     "iopub.status.busy": "2021-06-07T05:58:32.780754Z",
     "iopub.status.idle": "2021-06-07T05:58:32.79403Z",
     "shell.execute_reply": "2021-06-07T05:58:32.792561Z",
     "shell.execute_reply.started": "2021-06-07T05:58:32.781151Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "loss_fn = SparseCategoricalCrossentropy()\n",
    "optimizer = Adam()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:58:32.796684Z",
     "iopub.status.busy": "2021-06-07T05:58:32.796123Z",
     "iopub.status.idle": "2021-06-07T05:58:32.809456Z",
     "shell.execute_reply": "2021-06-07T05:58:32.807893Z",
     "shell.execute_reply.started": "2021-06-07T05:58:32.79664Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# Loss\n",
    "train_loss = Mean(name='train_loss')\n",
    "valid_loss = Mean(name='valid_loss')\n",
    "# Accuracy\n",
    "train_accuracy = SparseCategoricalAccuracy(name='train_accuracy')\n",
    "valid_accuracy = SparseCategoricalAccuracy(name='valid_accuracy')"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:58:32.812016Z",
     "iopub.status.busy": "2021-06-07T05:58:32.811425Z",
     "iopub.status.idle": "2021-06-07T05:58:32.854706Z",
     "shell.execute_reply": "2021-06-07T05:58:32.853505Z",
     "shell.execute_reply.started": "2021-06-07T05:58:32.811864Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "@tf.function\n",
    "def train_step(image, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Make a prediction on all the batch\n",
    "        predictions = model(image)\n",
    "        # Get the error/loss on these predictions\n",
    "        loss = loss_fn(targets, predictions)\n",
    "    # Compute the gradient which respect to the loss\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    # Change the weights of the model\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    # The metrics are accumulate over time. You don't need to average it yourself.\n",
    "    train_loss(loss)\n",
    "    train_accuracy(targets, predictions)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:58:32.856713Z",
     "iopub.status.busy": "2021-06-07T05:58:32.85636Z",
     "iopub.status.idle": "2021-06-07T05:58:32.86648Z",
     "shell.execute_reply": "2021-06-07T05:58:32.86518Z",
     "shell.execute_reply.started": "2021-06-07T05:58:32.856674Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "@tf.function\n",
    "def valid_step(image, targets):\n",
    "    predictions = model(image)\n",
    "    t_loss = loss_fn(targets, predictions)\n",
    "    # Set the metrics for the test\n",
    "    valid_loss(t_loss)\n",
    "    valid_accuracy(targets, predictions)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:58:32.870748Z",
     "iopub.status.busy": "2021-06-07T05:58:32.870309Z",
     "iopub.status.idle": "2021-06-07T05:58:32.87774Z",
     "shell.execute_reply": "2021-06-07T05:58:32.876379Z",
     "shell.execute_reply.started": "2021-06-07T05:58:32.870701Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, model=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, 'training_checkpoints/tf_ckpts', max_to_keep=10)\n",
    "ckpt.restore(manager.latest_checkpoint)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1bc2b58d3d0>"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:58:32.880025Z",
     "iopub.status.busy": "2021-06-07T05:58:32.879565Z",
     "iopub.status.idle": "2021-06-07T05:58:32.89966Z",
     "shell.execute_reply": "2021-06-07T05:58:32.898261Z",
     "shell.execute_reply.started": "2021-06-07T05:58:32.879984Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "epoch = 10\n",
    "batch_size = 64\n",
    "b = 0\n",
    "\n",
    "training_acc = []\n",
    "validation_acc = []\n",
    "for epoch in range(epoch):\n",
    "    # Training set\n",
    "    for images_batch, targets_batch in train_dataset.batch(batch_size):\n",
    "        train_step(images_batch, targets_batch)\n",
    "        template = '\\r Batch {}/{}, Loss: {}, Accuracy: {}'\n",
    "        print(template.format(\n",
    "            b, len(training_targets), train_loss.result(), \n",
    "            train_accuracy.result()*100\n",
    "        ), end=\"\")\n",
    "        b += batch_size\n",
    "    # Validation set\n",
    "    for images_batch, targets_batch in cv_dataset.batch(batch_size):\n",
    "        valid_step(images_batch, targets_batch)\n",
    "\n",
    "    template = '\\nEpoch {}, Valid Loss: {}, Valid Accuracy: {}'\n",
    "    print(template.format(\n",
    "        epoch+1,\n",
    "        valid_loss.result(), \n",
    "        valid_accuracy.result()*100)\n",
    "    )\n",
    "    training_acc.append(float(train_accuracy.result()*100))\n",
    "    validation_acc.append(float(valid_accuracy.result()*100))\n",
    "    ckpt.step.assign_add(1)\n",
    "    save_path = manager.save()\n",
    "    print(\"Saved checkpoint for step {}: {}\\n\\n\".format(int(ckpt.step), save_path))\n",
    "    valid_loss.reset_states()\n",
    "    valid_accuracy.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    train_loss.reset_states()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\KAMADS~1\\AppData\\Local\\Temp/ipykernel_5172/2386866212.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mimages_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mtemplate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\r Batch {}/{}, Loss: {}, Accuracy: {}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T05:58:32.901648Z",
     "iopub.status.busy": "2021-06-07T05:58:32.901307Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(manager.checkpoints)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# save the model for use in the application\n",
    "model.save_weights('weights/path_to_my_weights', save_format='tf')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot accuracy / epoch\n",
    "plt.plot([1,2,3,4,5,6,7,8,9,10],training_acc, '-' )\n",
    "plt.plot([1,2,3,4,5,6,7,8,9,10],validation_acc, '-' )\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4eaf1be304415beee96765ae99c3f893cc8312c7f1196698e6029668e9aeb3e5"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}