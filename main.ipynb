{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12038606236153424308\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return gray image\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pushing Two Fingers Away'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training targets\n",
    "targets = pd.read_csv('data_csv/jester-v1-train.csv', header=None, index_col=0, squeeze=True).to_dict()\n",
    "targets[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Drumming Fingers'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation targets\n",
    "targets_validation = pd.read_csv('data_csv/jester-v1-validation.csv', header=None, index_col=0, squeeze=True).to_dict()\n",
    "targets_validation[10965]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50420"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets)\n",
    "# targets[50420]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Doing other things</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drumming Fingers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No gesture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pulling Hand In</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pulling Two Fingers In</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0\n",
       "0      Doing other things\n",
       "1        Drumming Fingers\n",
       "2              No gesture\n",
       "3         Pulling Hand In\n",
       "4  Pulling Two Fingers In"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classes label you want to use all labels \n",
    "label = pd.read_csv('data_csv/jester-v1-labels.csv',header=None, usecols=[0])\n",
    "label.head()\n",
    "# The classes (labels) we want to use\n",
    "# targets_name = [\n",
    "#     \"Swiping Right\",\n",
    "#     \"Sliding Two Fingers Left\",\n",
    "#     \"No gesture\",\n",
    "#     \"Thumb Up\"\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Doing other things',\n",
       " 'Drumming Fingers',\n",
       " 'No gesture',\n",
       " 'Pulling Hand In',\n",
       " 'Pulling Two Fingers In',\n",
       " 'Pushing Hand Away',\n",
       " 'Pushing Two Fingers Away',\n",
       " 'Rolling Hand Backward',\n",
       " 'Rolling Hand Forward',\n",
       " 'Shaking Hand',\n",
       " 'Sliding Two Fingers Down',\n",
       " 'Sliding Two Fingers Left',\n",
       " 'Sliding Two Fingers Right',\n",
       " 'Sliding Two Fingers Up',\n",
       " 'Stop Sign',\n",
       " 'Swiping Down',\n",
       " 'Swiping Left',\n",
       " 'Swiping Right',\n",
       " 'Swiping Up',\n",
       " 'Thumb Down',\n",
       " 'Thumb Up',\n",
       " 'Turning Hand Clockwise',\n",
       " 'Turning Hand Counterclockwise',\n",
       " 'Zooming In With Full Hand',\n",
       " 'Zooming In With Two Fingers',\n",
       " 'Zooming Out With Full Hand',\n",
       " 'Zooming Out With Two Fingers']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_name = label[0].tolist()\n",
    "targets_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data directories\n",
    "path = \"20/\"\n",
    "path_cv = \"validation_samples/\"\n",
    "\n",
    "dirs = os.listdir(path)\n",
    "dirs_cv = os.listdir(path_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples for training and validation\n",
    "print(len(dirs))\n",
    "print(len(dirs_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_count = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The videos do not have the same number of frames, here we try to unify.\n",
    "'''\n",
    "hm_frames = 30 # number of frames\n",
    "# unify number of frames for each training\n",
    "def get_unify_frames(path):\n",
    "    offset = 0\n",
    "    # pick frames\n",
    "    frames = os.listdir(path)\n",
    "    frames_count = len(frames)\n",
    "    # unify number of frames \n",
    "    if hm_frames > frames_count:\n",
    "        # duplicate last frame if video is shorter than necessary\n",
    "        frames += [frames[-1]] * (hm_frames - frames_count)\n",
    "    elif hm_frames < frames_count:\n",
    "        # If there are more frames, then sample starting offset\n",
    "        #diff = (frames_count - hm_frames)\n",
    "        #offset = diff-1 \n",
    "        frames = frames[0:hm_frames]\n",
    "    return frames  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize frames\n",
    "def resize_frame(frame):\n",
    "    frame = img.imread(frame)\n",
    "    frame = cv2.resize(frame, (64, 64))\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust training data\n",
    "counter_training = 0 # number of training\n",
    "training_targets = [] # training targets \n",
    "new_frames = [] # training data after resize & unify\n",
    "\n",
    "for directory in dirs:\n",
    "    new_frame = [] # one training\n",
    "    # Frames in each folder\n",
    "    frames = get_unify_frames(path+directory)\n",
    "    if len(frames) == hm_frames: # just to be sure\n",
    "        for frame in frames:\n",
    "            frame = resize_frame(path+directory+'/'+frame)\n",
    "            new_frame.append(rgb2gray(frame))\n",
    "            if len(new_frame) == 15: # partition each training on two trainings.\n",
    "                new_frames.append(new_frame) # append each partition to training data\n",
    "                training_targets.append(targets_name.index(targets[int(directory)]))\n",
    "                counter_training +=1\n",
    "                new_frame = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do the same for the validation data\n",
    "counter_validation = 0\n",
    "cv_targets = []\n",
    "new_frames_cv = []\n",
    "\n",
    "for directory in dirs_cv:\n",
    "    new_frame = []\n",
    "    # Frames in each folder\n",
    "    frames = get_unify_frames(path_cv+directory)\n",
    "    if len(frames)==hm_frames:\n",
    "        for frame in frames:\n",
    "            frame = resize_frame(path_cv+directory+'/'+frame)\n",
    "            new_frame.append(rgb2gray(frame))\n",
    "            if len(new_frame) == 15:\n",
    "                new_frames_cv.append(new_frame)\n",
    "                cv_targets.append(targets_name.index(targets_validation[int(directory)]))\n",
    "                counter_validation +=1\n",
    "                new_frame = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check training length\n",
    "print(len(new_frames))\n",
    "print(len(training_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check validation length\n",
    "print(len(new_frames_cv))\n",
    "print(len(cv_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_targets[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show data\n",
    "fig = plt.figure()\n",
    "for i in range(2,4):\n",
    "    for num,frame in enumerate(new_frames[i][0:18]):\n",
    "        y = fig.add_subplot(4,5,num+1)\n",
    "        y.imshow(frame, cmap='gray')\n",
    "    fig = plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert training data to np float32\n",
    "training_data = np.array(new_frames[0:counter_training], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to empty the RAM\n",
    "def release_list(a):\n",
    "   del a[:]\n",
    "   del a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_list(new_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert validation data to np float32\n",
    "cv_data = np.array(new_frames_cv[0:counter_validation], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_list(new_frames_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation: training\n",
    "print('old mean', training_data.mean())\n",
    "scaler = StandardScaler()\n",
    "scaled_images  = scaler.fit_transform(training_data.reshape(-1, 15*64*64))\n",
    "print('new mean', scaled_images.mean())\n",
    "scaled_images  = scaled_images.reshape(-1, 15, 64, 64, 1)\n",
    "print(scaled_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation: validation\n",
    "print('old mean', cv_data.mean())\n",
    "scaler = StandardScaler()\n",
    "scaled_images_cv  = scaler.fit_transform(cv_data.reshape(-1, 15*64*64))\n",
    "print('new mean',scaled_images_cv.mean())\n",
    "scaled_images_cv  = scaled_images_cv.reshape(-1, 15, 64, 64, 1)\n",
    "print(scaled_images_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My model\n",
    "class Conv3DModel(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Conv3DModel, self).__init__()\n",
    "    # Convolutions\n",
    "    self.conv1 = tf.compat.v2.keras.layers.Conv3D(32, (3, 3, 3), activation='relu', name=\"conv1\", data_format='channels_last')\n",
    "    self.pool1 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), data_format='channels_last')\n",
    "    self.conv2 = tf.compat.v2.keras.layers.Conv3D(64, (3, 3, 3), activation='relu', name=\"conv1\", data_format='channels_last')\n",
    "    self.pool2 = tf.keras.layers.MaxPool3D(pool_size=(2, 2,2), data_format='channels_last')\n",
    "   \n",
    "    # LSTM & Flatten\n",
    "    self.convLSTM =tf.keras.layers.ConvLSTM2D(40, (3, 3))\n",
    "    self.flatten =  tf.keras.layers.Flatten(name=\"flatten\")\n",
    "\n",
    "    # Dense layers\n",
    "    self.d1 = tf.keras.layers.Dense(128, activation='relu', name=\"d1\")\n",
    "    self.out = tf.keras.layers.Dense(4, activation='softmax', name=\"output\")\n",
    "    \n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.pool1(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.pool2(x)\n",
    "    x = self.convLSTM(x)\n",
    "    #x = self.pool2(x)\n",
    "    #x = self.conv3(x)\n",
    "    #x = self.pool3(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.d1(x)\n",
    "    return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. the simplest method with beginner syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv3DModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the loss and optimizer methods\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(scaled_images)\n",
    "y_train = np.array(training_targets)\n",
    "x_val = np.array(scaled_images_cv)\n",
    "y_val = np.array(cv_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### here I use the checkpoints\n",
    "read more:\n",
    "https://www.tensorflow.org/beta/guide/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include the epoch in the file name. (uses `str.format`)\n",
    "checkpoint_path = \"training_today/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, verbose=1, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training \n",
    "history = model.fit(x_train, y_train,\n",
    "                    callbacks = [cp_callback],\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    batch_size=32,\n",
    "                    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just after one epoch\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model for use in the application\n",
    "model.save_weights('weights/path_to_my_weights', save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. use a more complex syntax, for experts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My model\n",
    "class Conv3DModel(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Conv3DModel, self).__init__()\n",
    "    # Convolutions\n",
    "    self.conv1 = tf.compat.v2.keras.layers.Conv3D(32, (3, 3, 3), activation='relu', name=\"conv1\", data_format='channels_last')\n",
    "    self.pool1 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), data_format='channels_last')\n",
    "    self.conv2 = tf.compat.v2.keras.layers.Conv3D(64, (3, 3, 3), activation='relu', name=\"conv1\", data_format='channels_last')\n",
    "    self.pool2 = tf.keras.layers.MaxPool3D(pool_size=(2, 2,2), data_format='channels_last')\n",
    "   \n",
    "    # LSTM & Flatten\n",
    "    self.convLSTM =tf.keras.layers.ConvLSTM2D(40, (3, 3))\n",
    "    self.flatten =  tf.keras.layers.Flatten(name=\"flatten\")\n",
    "\n",
    "    # Dense layers\n",
    "    self.d1 = tf.keras.layers.Dense(128, activation='relu', name=\"d1\")\n",
    "    self.out = tf.keras.layers.Dense(4, activation='softmax', name=\"output\")\n",
    "    \n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.pool1(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.pool2(x)\n",
    "    x = self.convLSTM(x)\n",
    "    #x = self.pool2(x)\n",
    "    #x = self.conv3(x)\n",
    "    #x = self.pool3(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.d1(x)\n",
    "    return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv3DModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tensorflow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((scaled_images, training_targets))\n",
    "cv_dataset = tf.data.Dataset.from_tensor_slices((scaled_images_cv, cv_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(scaled_images[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n",
    "# Accuracy\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "valid_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='valid_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(image, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Make a prediction on all the batch\n",
    "        predictions = model(image)\n",
    "        # Get the error/loss on these predictions\n",
    "        loss = loss_fn(targets, predictions)\n",
    "    # Compute the gradient which respect to the loss\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    # Change the weights of the model\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    # The metrics are accumulate over time. You don't need to average it yourself.\n",
    "    train_loss(loss)\n",
    "    train_accuracy(targets, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def valid_step(image, targets):\n",
    "    predictions = model(image)\n",
    "    t_loss = loss_fn(targets, predictions)\n",
    "    # Set the metrics for the test\n",
    "    valid_loss(t_loss)\n",
    "    valid_accuracy(targets, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### here I use the checkpoints\n",
    "read more:\n",
    "https://www.tensorflow.org/beta/guide/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, model=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, 'training_checkpoints/tf_ckpts', max_to_keep=10)\n",
    "ckpt.restore(manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "batch_size = 32\n",
    "b = 0\n",
    "training_acc = []\n",
    "validation_acc = []\n",
    "for epoch in range(epoch):\n",
    "    # Training set\n",
    "    for images_batch, targets_batch in train_dataset.batch(batch_size):\n",
    "        train_step(images_batch, targets_batch)\n",
    "        template = '\\r Batch {}/{}, Loss: {}, Accuracy: {}'\n",
    "        print(template.format(\n",
    "            b, len(training_targets), train_loss.result(), \n",
    "            train_accuracy.result()*100\n",
    "        ), end=\"\")\n",
    "        b += batch_size\n",
    "    # Validation set\n",
    "    for images_batch, targets_batch in cv_dataset.batch(batch_size):\n",
    "        valid_step(images_batch, targets_batch)\n",
    "\n",
    "    template = '\\nEpoch {}, Valid Loss: {}, Valid Accuracy: {}'\n",
    "    print(template.format(\n",
    "        epoch+1,\n",
    "        valid_loss.result(), \n",
    "        valid_accuracy.result()*100)\n",
    "    )\n",
    "    training_acc.append(float(train_accuracy.result()*100))\n",
    "    validation_acc.append(float(valid_accuracy.result()*100))\n",
    "    ckpt.step.assign_add(1)\n",
    "    save_path = manager.save()\n",
    "    print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))\n",
    "    valid_loss.reset_states()\n",
    "    valid_accuracy.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    train_loss.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(manager.checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plote Accuracy / epoch\n",
    "plt.plot([1,2,3,4,5,6,7,8,9],training_acc, '-' )\n",
    "plt.plot([1,2,3,4,5,6,7,8,9],validation_acc, '-' )\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model for use in the application\n",
    "model.save_weights('weights/path_to_my_weights', save_format='tf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "2fc04cdb9730b0f31da2f90a691cdd0fa9da7bc1222eb1adb5ee73ed7bedeaa9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
